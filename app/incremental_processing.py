"""
Inkrement√°lis adatfeldolgoz√°s - csak √∫j/m√≥dosult adatok feldolgoz√°sa.
Timestamp √©s hash-based change detection.
"""

import os
import json
import hashlib
import pandas as pd
from datetime import datetime
from typing import Dict, Set, Tuple, Optional
from pathlib import Path


class IncrementalProcessor:
    """
    Inkrement√°lis feldolgoz√°s kezel≈ë oszt√°ly.
    Timestamp √©s checksum alap√∫ v√°ltoz√°s detekt√°l√°s.
    """
    
    def __init__(self, metadata_file: str = '/workspace/processing_metadata.json'):
        """
        Inicializ√°l√°s.
        
        Args:
            metadata_file: Metadata f√°jl el√©r√©si √∫tja (JSON)
        """
        self.metadata_file = metadata_file
        self.metadata = self._load_metadata()
    
    def _load_metadata(self) -> Dict:
        """Metadata bet√∂lt√©se f√°jlb√≥l (ha l√©tezik)."""
        if os.path.exists(self.metadata_file):
            try:
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"‚ö†Ô∏è Metadata bet√∂lt√©si hiba: {e}", flush=True)
                return self._default_metadata()
        return self._default_metadata()
    
    def _default_metadata(self) -> Dict:
        """Default metadata strukt√∫ra."""
        return {
            'last_processing_timestamp': None,
            'last_processing_date': None,
            'total_processed': 0,
            'article_checksums': {}  # {article_id: sha256_hash}
        }
    
    def _save_metadata(self):
        """Metadata ment√©se f√°jlba."""
        try:
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, indent=2, ensure_ascii=False)
            print(f"üíæ Metadata mentve: {self.metadata_file}", flush=True)
        except Exception as e:
            print(f"‚ö†Ô∏è Metadata ment√©si hiba: {e}", flush=True)
    
    def compute_article_hash(self, article_data: pd.Series) -> str:
        """
        Cikk hash sz√°m√≠t√°sa (checksum a v√°ltoz√°s detekt√°l√°shoz).
        
        Args:
            article_data: Article sor (pandas Series)
            
        Returns:
            SHA256 hash (hex string)
        """
        # Kulcs mez≈ëk a hash sz√°m√≠t√°shoz
        key_fields = ['description', 'title', 'price_huf', 'area_sqm', 'district']
        
        # √ârt√©kek √∂sszef≈±z√©se
        data_str = ""
        for field in key_fields:
            value = article_data.get(field, '')
            if pd.notna(value):
                data_str += str(value)
        
        # SHA256 hash
        return hashlib.sha256(data_str.encode('utf-8')).hexdigest()
    
    def filter_new_and_changed(
        self, 
        df: pd.DataFrame,
        timestamp_column: str = 'delivery_day',
        force_reprocess: bool = False
    ) -> Tuple[pd.DataFrame, Dict[str, str]]:
        """
        Sz≈±r√©s: csak √∫j vagy m√≥dosult cikkek.
        
        Args:
            df: Input DataFrame (teljes adathalmaz)
            timestamp_column: Timestamp oszlop neve
            force_reprocess: Ha True, minden cikket √∫jrafeldolgoz
            
        Returns:
            Tuple: (Sz≈±rt DataFrame csak √∫j/m√≥dosult cikkekkel, √öj checksumok dict)
        """
        if force_reprocess:
            print("üîÑ Force reprocess mode: minden cikk feldolgoz√°sra ker√ºl", flush=True)
            new_checksums = {}
            for _, row in df.iterrows():
                article_id = row['article_id']
                new_checksums[article_id] = self.compute_article_hash(row)
            return df, new_checksums
        
        last_timestamp = self.metadata.get('last_processing_timestamp')
        existing_checksums = self.metadata.get('article_checksums', {})
        
        print(f"üìä Inkrement√°lis sz≈±r√©s ind√≠t√°sa...", flush=True)
        print(f"   Utols√≥ feldolgoz√°s: {self.metadata.get('last_processing_date', 'Soha')}", flush=True)
        print(f"   Kor√°bban feldolgozott cikkek: {len(existing_checksums)}", flush=True)
        
        new_articles = []
        changed_articles = []
        unchanged_articles = []
        new_checksums = {}
        
        for idx, row in df.iterrows():
            article_id = row['article_id']
            current_hash = self.compute_article_hash(row)
            new_checksums[article_id] = current_hash
            
            # 1. √öj cikk (m√©g nem volt feldolgozva)
            if article_id not in existing_checksums:
                new_articles.append(idx)
                continue
            
            # 2. M√≥dosult cikk (hash v√°ltoz√°s)
            if existing_checksums[article_id] != current_hash:
                changed_articles.append(idx)
                continue
            
            # 3. V√°ltozatlan cikk
            unchanged_articles.append(idx)
        
        print(f"‚úÖ Sz≈±r√©si eredm√©ny:", flush=True)
        print(f"   üÜï √öj cikkek: {len(new_articles)}", flush=True)
        print(f"   üîÑ M√≥dosult cikkek: {len(changed_articles)}", flush=True)
        print(f"   ‚úì V√°ltozatlan cikkek: {len(unchanged_articles)}", flush=True)
        
        # Csak az √∫j √©s m√≥dosult cikkek DataFrame-je
        filtered_indices = new_articles + changed_articles
        filtered_df = df.loc[filtered_indices].copy() if filtered_indices else pd.DataFrame()
        
        return filtered_df, new_checksums
    
    def update_metadata(self, new_checksums: Dict[str, str], processed_count: int):
        """
        Metadata friss√≠t√©se feldolgoz√°s ut√°n.
        
        Args:
            new_checksums: √öj cikk checksumok dictionary
            processed_count: Feldolgozott cikkek sz√°ma
        """
        current_time = datetime.now()
        
        # Checksumok friss√≠t√©se (merge √∫j + megl√©v≈ë)
        self.metadata['article_checksums'].update(new_checksums)
        
        # Timestamp friss√≠t√©se
        self.metadata['last_processing_timestamp'] = current_time.timestamp()
        self.metadata['last_processing_date'] = current_time.strftime('%Y-%m-%d %H:%M:%S')
        
        # √ñsszes√≠tett statisztik√°k
        self.metadata['total_processed'] = len(self.metadata['article_checksums'])
        
        # Ment√©s
        self._save_metadata()
        
        print(f"üìà Metadata friss√≠tve:", flush=True)
        print(f"   Feldolgozva most: {processed_count}", flush=True)
        print(f"   √ñsszes cikk: {self.metadata['total_processed']}", flush=True)
    
    def get_stats(self) -> Dict:
        """
        Inkrement√°lis feldolgoz√°s statisztik√°k.
        
        Returns:
            Dictionary statisztik√°kkal
        """
        return {
            'last_processing_date': self.metadata.get('last_processing_date'),
            'total_articles_tracked': len(self.metadata.get('article_checksums', {})),
            'metadata_file': self.metadata_file,
            'metadata_exists': os.path.exists(self.metadata_file)
        }
    
    def reset_metadata(self):
        """Metadata t√∂rl√©se (teljes √∫jrafeldolgoz√°shoz)."""
        self.metadata = self._default_metadata()
        if os.path.exists(self.metadata_file):
            os.remove(self.metadata_file)
        print("üóëÔ∏è Metadata t√∂r√∂lve - k√∂vetkez≈ë futtat√°s teljes feldolgoz√°s lesz", flush=True)


# Global singleton instance
_incremental_processor = None

def get_incremental_processor() -> IncrementalProcessor:
    """
    Singleton IncrementalProcessor instance lek√©r√©se.
    
    Returns:
        IncrementalProcessor instance
    """
    global _incremental_processor
    if _incremental_processor is None:
        _incremental_processor = IncrementalProcessor()
    return _incremental_processor
