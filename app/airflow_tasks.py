"""
Airflow-kompatibilis adatfeldolgoz√≥ f√ºggv√©nyek.
Ezek a f√ºggv√©nyek helyettes√≠tik a r√©gi RQ-alap√∫ background_tasks.py-t.
"""

import pandas as pd
import asyncio
import aiohttp
from typing import List, Dict, Any
import sys
import os

# Projekt k√∂nyvt√°r hozz√°ad√°sa
sys.path.insert(0, '/workspace/app')

from llm_cache import get_cached_result, set_cached_result
from models import sanitize_llm_output
import ollama

MODEL_NAME = os.getenv('LLM_MODEL', 'llama3.2:3b')
OLLAMA_URL = os.getenv('OLLAMA_URL', 'http://ollama:11434')
LOG_FILE = '/workspace/llm_decisions_log.csv'


async def async_ollama_chat(session: aiohttp.ClientSession, prompt: str, model: str = MODEL_NAME) -> Dict[str, Any]:
    """Aszinkron LLM h√≠v√°s"""
    url = f"{OLLAMA_URL}/api/chat"
    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "stream": False,
        "options": {"temperature": 0.0}
    }
    
    try:
        async with session.post(url, json=payload, timeout=aiohttp.ClientTimeout(total=300)) as response:
            response.raise_for_status()
            result = await response.json()
            return result
    except Exception as e:
        print(f"‚ùå LLM h√≠v√°s hiba: {e}")
        return None


async def async_process_single_article(session: aiohttp.ClientSession, article: Dict) -> Dict:
    """Egyetlen cikk feldolgoz√°sa cache-el√©ssel"""
    description = str(article.get('description', ''))
    article_id = article.get('article_id', 'unknown')
    
    # Cache ellen≈ërz√©s
    cached = get_cached_result(description)
    if cached:
        print(f"üíæ Cache hit: {article_id}")
        return {**article, **cached}
    
    # LLM h√≠v√°s
    prompt = f"""
Elemezd ezt a budapesti ingatlanhirdet√©st:

Le√≠r√°s: {description}

Relev√°ns: B√ÅRMILYEN ingatlan Budapesten (lak√°s, h√°z, telek, gar√°zs stb.)
Irrelev√°ns: Tulajdoni h√°nyad, b√©rleti jog, haszon√©lvezet, csere

JSON v√°lasz:
{{"relevant": true/false, "reason": "indokl√°s"}}
"""
    
    response = await async_ollama_chat(session, prompt)
    if not response:
        return {**article, 'relevant': False, 'reason': 'LLM hiba'}
    
    # Parse eredm√©ny
    try:
        content = response['message']['content']
        sanitized = sanitize_llm_output(content)
        import json
        result = json.loads(sanitized)
        
        # Cache ment√©s
        set_cached_result(description, result)
        
        return {**article, **result}
    except Exception as e:
        print(f"‚ö†Ô∏è Parse hiba {article_id}: {e}")
        return {**article, 'relevant': False, 'reason': 'Parse hiba'}


async def async_process_articles_batch(df: pd.DataFrame) -> List[Dict]:
    """
    Teljes adathalmaz feldolgoz√°sa batch-ekben.
    Airflow task sz√°m√°ra optimaliz√°lt.
    """
    results = []
    
    async with aiohttp.ClientSession() as session:
        # Batch-ek l√©trehoz√°sa (50 cikk/batch p√°rhuzamos feldolgoz√°sra)
        batch_size = 50
        for i in range(0, len(df), batch_size):
            batch = df.iloc[i:i+batch_size]
            
            # P√°rhuzamos feldolgoz√°s aszinkron m√≥don
            tasks = [async_process_single_article(session, row) for _, row in batch.iterrows()]
            batch_results = await asyncio.gather(*tasks)
            results.extend(batch_results)
            
            print(f"‚úÖ Feldolgozva: {len(results)}/{len(df)} cikk")
    
    return results


def save_llm_decisions_to_log(results: List[Dict]):
    """LLM d√∂nt√©sek ment√©se CSV-be (ML tr√©ninghez)"""
    import csv
    
    with open(LOG_FILE, 'a', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=['article_id', 'description', 'relevant', 'reason'])
        
        for result in results:
            writer.writerow({
                'article_id': result.get('article_id'),
                'description': result.get('description', ''),
                'relevant': result.get('relevant'),
                'reason': result.get('reason', '')
            })
    
    print(f"üìù {len(results)} LLM d√∂nt√©s mentve: {LOG_FILE}")
