"""
H√°tt√©rfeladatok v√©grehajt√°sa aszinkron m√≥don.
Ez a modul tartalmazza az adatfeldolgoz√°si logik√°t RQ worker sz√°m√°ra.
"""

import pandas as pd
import ollama
import json
import os
import re
import sys
import asyncio
import aiohttp
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Tuple, Dict, Any, List
from task_manager import TaskManager
from models import sanitize_llm_output, validate_dataframe_schema, PropertyInput
from llm_cache import get_cached_result, set_cached_result, get_cache_stats, test_cache_connection
from ml_worker_filter import get_ml_filter, train_ml_filter_from_llm_log
from parquet_streaming import (
    ParquetStreamReader, 
    get_unique_articles_streaming, 
    estimate_parquet_memory
)
from connection_pool import get_ollama_session, get_connection_pool_stats
from incremental_processing import get_incremental_processor

# Constants
MODEL_NAME = os.getenv('LLM_MODEL', 'llama3.2:3b')
INPUT_FILE = '/workspace/core_data.parquet'
OUTPUT_FILE = '/workspace/parquet/core_layer_filtered.parquet'
IRRELEVANT_OUTPUT_FILE = '/workspace/parquet/core_layer_irrelevant.parquet'
LOG_FILE = '/workspace/llm_decisions_log.csv'

# üß™ TESZT M√ìD: Csak els≈ë 100 hirdet√©st dolgozza fel (True = teszt, False = teljes)
TEST_MODE = True
TEST_LIMIT = 100

# Batch LLM feldolgoz√°shoz (3 cikk egyszerre)
BATCH_PROMPT_TEMPLATE = """
Elemezd az al√°bbi {count} budapesti ingatlanhirdet√©st lak√°sv√°s√°rl√°s szempontj√°b√≥l.
FONTOS: Mindegyiket K√úL√ñN-K√úL√ñN √©rt√©keld, ne keverd ≈ëket √∂ssze!

CIKK #1:
ID: {id_1}
Le√≠r√°s: {desc_1}

CIKK #2:
ID: {id_2} 
Le√≠r√°s: {desc_2}

CIKK #3:
ID: {id_3}
Le√≠r√°s: {desc_3}

Szempontok:
- Relev√°ns: Lak√°s, t√°rsash√°zi lak√°s, csal√°di h√°z, ikerh√°z Budapesten - f≈ëv√°rosban lak√°svev≈ëknek √©rdekes
- Irrelev√°ns: Tulajdoni h√°nyad, nyaral√≥, telek, gar√°zs, √ºzlethelyis√©g, ker√ºlend≈ë konstrukci√≥k

V√ÅLASZFORM√ÅTUM (JSON array, pontosan 3 elem):
[
  {{"id": "{id_1}", "relevant": true, "reason": "r√∂vid indokl√°s", "floor": null, "street": null, "building_type": null, "property_category": null, "has_terrace": null}},
  {{"id": "{id_2}", "relevant": false, "reason": "r√∂vid indokl√°s", "floor": null, "street": null, "building_type": null, "property_category": null, "has_terrace": null}},
  {{"id": "{id_3}", "relevant": true, "reason": "r√∂vid indokl√°s", "floor": null, "street": null, "building_type": null, "property_category": null, "has_terrace": null}}
]
"""

# Egyedi cikk feldolgoz√°shoz (fallback)
PROMPT_TEMPLATE = """
Feladat: Ingatlanle√≠r√°s alapj√°n d√∂ntsd el a relevanci√°t √©s nyerd ki a struktur√°lt adatokat.
Alap√©rtelmez√©s: Az ingatlan relev√°ns (true), kiv√©ve, ha kiz√°r√≥ okot tal√°lsz.

Kiz√°r√≥ okok (`relevant: false`):
- Nem 1/1 tulajdon vagy nem tiszta elad√°s (pl. osztatlan k√∂z√∂s, tulajdoni h√°nyad, b√©rleti jog, haszon√©lvezet, √∂nkorm√°nyzati, csere).
- Ingatlan t√≠pusa: Kiz√°r√≥ ok, ha NEM lak√°s c√©l√∫ (telek, gar√°zs, nyaral√≥, √ºd√ºl≈ë NEM relev√°ns). Lak√°s, csal√°di h√°z, ikerh√°z Budapesten RELEV√ÅNS.
- Nem budapesti.
- "Csak k√©szp√©nzes vev≈ëknek" (ez gyakran jogi probl√©m√°t jelez).

Struktur√°lt adatok:
- Emelet (`floor`): Sz√°m (pl. 1, 2). F√∂ldszint: 0. Szuter√©n/f√©lszuter√©n: -1. Ha nincs info: null.
- Utca (`street`): Az ingatlan utcaneve (pl. 'Kossuth Lajos utca'). Ha nincs: null.
- √âp√≠t√©si m√≥d (`building_type`): "tegla" ha t√©gla √©p√ºlet, "panel" ha panelh√°z, "egyeb" ha m√°s (pl. v√°lyog, fav√°zas), null ha nincs info.
- Ingatlan kateg√≥ria (`property_category`): "lakas" vagy "haz". A t√°rsash√°zi lak√°s = "lakas". Ha nincs info: null.
- Terasz (`has_terrace`): true ha van terasz/erk√©ly/loggia/franciaerk√©ly, false ha nincs vagy nem eml√≠ti, null ha nem egy√©rtelm≈±.

Le√≠r√°s: {description}

Form√°tum: A v√°laszod CSAK egy JSON objektum legyen, extra sz√∂veg n√©lk√ºl.
P√©lda: {{"relevant": true, "reason": "", "floor": 1, "street": "Eg√©szs√©gh√°z utca", "building_type": "tegla", "property_category": "lakas", "has_terrace": true}}
vagy {{"relevant": false, "reason": "Csal√°di h√°z", "floor": null, "street": null, "building_type": null, "property_category": "haz", "has_terrace": null}}
"""

# Fejlett szab√°lyalap√∫ sz≈±r√©s - k√©tl√©pcs≈ës megk√∂zel√≠t√©ssel
DEFINITELY_IRRELEVANT_KEYWORDS = {
    "tulajdoni h√°nyad": "Tulajdoni h√°nyad", 
    "b√©rleti jog": "B√©rleti jog (nem elad√°s)",
    "√∂nkorm√°nyzati ingatlan": "√ñnkorm√°nyzati ingatlan",
    "ingatlancsere": "Ingatlancsere (nem elad√°s)",
    "cser√©lhet≈ë": "Csere (nem elad√°s)",
    "cser√©ln√©m": "Csere (nem elad√°s)",
    "haszon√©lvezet": "Haszon√©lvezet",
    "haszon√©lvezettel": "Haszon√©lvezet",
}

LIKELY_IRRELEVANT_KEYWORDS = {
    # √úres - minden m√°s ingatlan t√≠pus enged√©lyezett (ikerh√°z, csal√°di h√°z, √ºzlet, iroda, nyaral√≥)
}

NEGATION_KEYWORDS = [
    "tehermentes", "per- √©s tehermentes", "per √©s tehermentes",
    "nincs haszon√©lvezet", "t√∂r√∂lve", "megsz√ºntetve",
    "nincs", "nem terhelt", "teljes tulajdon", "1/1 tulajdon",
    "mag√°ntulajdon", "mag√°nszem√©ly tulajdonban"
]

# ============================================================================
# ASYNC LLM H√çV√ÅSOK AIOHTTP-VAL
# ============================================================================

OLLAMA_URL = os.getenv('OLLAMA_URL', 'http://ollama:11434')  # Docker bels≈ë h√°l√≥zaton

async def async_ollama_chat(session: aiohttp.ClientSession, prompt: str, model: str = MODEL_NAME) -> Dict[str, Any]:
    """
    Aszinkron LLM h√≠v√°s aiohttp-val.
    
    Args:
        session: aiohttp ClientSession
        prompt: Az LLM-nek k√ºld√∂tt prompt
        model: A haszn√°lt model neve
        
    Returns:
        Az LLM v√°lasza dictionary form√°ban
    """
    url = f"{OLLAMA_URL}/api/chat"
    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "stream": False,
        "options": {"temperature": 0.0}
    }
    
    try:
        async with session.post(url, json=payload, timeout=aiohttp.ClientTimeout(total=300)) as response:
            response.raise_for_status()
            result = await response.json()
            return result
    except asyncio.TimeoutError:
        raise Exception(f"LLM h√≠v√°s timeout (300s) - model: {model}")
    except aiohttp.ClientError as e:
        raise Exception(f"LLM h√≠v√°s hiba: {e}")

async def async_get_batch_llm_decision(session: aiohttp.ClientSession, articles_batch: List[pd.Series]) -> List[Dict[str, Any]]:
    """
    Aszinkron batch LLM feldolgoz√°s 3 cikkhez.
    
    Args:
        session: aiohttp ClientSession
        articles_batch: 3 cikk list√°ja
        
    Returns:
        Lista a 3 cikk eredm√©ny√©vel
    """
    if len(articles_batch) != 3:
        raise ValueError("Batch size must be exactly 3")
    
    try:
        combined_texts = []
        article_ids = []
        
        for article in articles_batch:
            article_id = article['article_id']
            description = article.get('description', '') or ''
            title = article.get('title', '') or ''
            combined_text = f"{title} {description}".strip()
            
            combined_texts.append(combined_text)
            article_ids.append(article_id)
        
        prompt = BATCH_PROMPT_TEMPLATE.format(
            count=3,
            id_1=article_ids[0], desc_1=combined_texts[0],
            id_2=article_ids[1], desc_2=combined_texts[1], 
            id_3=article_ids[2], desc_3=combined_texts[2]
        )
        
        print(f"üöÄ ASYNC BATCH LLM h√≠v√°s: {article_ids}", flush=True)
        
        # Async LLM h√≠v√°s
        response = await async_ollama_chat(session, prompt)
        content = response['message']['content']
        
        # JSON array parsing
        json_match = re.search(r'\[.*\]', content, re.DOTALL)
        if not json_match:
            raise ValueError("Nem tal√°lhat√≥ JSON array a v√°laszban")
        
        results_array = json.loads(json_match.group(0))
        
        if len(results_array) != 3:
            raise ValueError(f"V√°rt 3 eredm√©ny, kapott: {len(results_array)}")
        
        # Valid√°l√°s
        from models import LLMResponse
        validated_results = []
        for result in results_array:
            try:
                validated = LLMResponse(**result)
                validated_results.append(validated.dict())
            except Exception as e:
                validated_results.append({
                    "relevant": False, 
                    "reason": f"Valid√°ci√≥s hiba: {str(e)}", 
                    "floor": None, "street": None, "building_type": None, 
                    "property_category": None, "has_terrace": None
                })
        
        print(f"‚úÖ ASYNC BATCH eredm√©ny: {len(validated_results)} cikk", flush=True)
        return validated_results
        
    except Exception as e:
        print(f"‚ùå ASYNC BATCH hiba: {e} - Fallback egyenk√©nti feldolgoz√°sra", flush=True)
        # Fallback: egyenk√©nti feldolgoz√°s
        individual_results = []
        for article in articles_batch:
            individual_result = await async_get_llm_decision_with_validation(
                session,
                f"{article.get('title', '')} {article.get('description', '')}".strip()
            )
            individual_results.append(individual_result)
        return individual_results

async def async_get_llm_decision_with_validation(session: aiohttp.ClientSession, description: str) -> Dict[str, Any]:
    """
    Aszinkron LLM h√≠v√°s egyedi cikk elemz√©s√©hez cache-el√©ssel.
    
    Args:
        session: aiohttp ClientSession
        description: Az ingatlanhirdet√©s le√≠r√°sa
        
    Returns:
        Dict az LLM d√∂nt√©ssel √©s struktur√°lt adatokkal
    """
    # 1. Cache ellen≈ërz√©s (szinkron)
    cached_result = get_cached_result(description)
    if cached_result:
        return cached_result
    
    # 2. LLM h√≠v√°s aszinkron m√≥don
    prompt = PROMPT_TEMPLATE.format(description=description)
    
    try:
        response = await async_ollama_chat(session, prompt)
        content = response['message']['content']
        
        # Sanitiz√°lt √©s valid√°lt kimenet
        from models import LLMResponse
        parsed_result = sanitize_llm_output(content)
        validated_result = LLMResponse(**parsed_result)
        result_dict = validated_result.dict()
        
        # 3. Cache ment√©s (szinkron)
        set_cached_result(description, result_dict)
        
        return result_dict
        
    except Exception as e:
        print(f"‚ùå ASYNC LLM hiba: {e}", flush=True)
        return {
            "relevant": False,
            "reason": f"LLM h√≠v√°s sikertelen: {str(e)}",
            "floor": None, "street": None, "building_type": None,
            "property_category": None, "has_terrace": None
        }

# ============================================================================
# SZINKRON WRAPPER F√úGGV√âNYEK (backward compatibility)
# ============================================================================

def get_batch_llm_decision(articles_batch: List[pd.Series]) -> List[Dict[str, Any]]:
    """
    Szinkron wrapper az async_get_batch_llm_decision f√ºggv√©nyhez.
    ThreadPoolExecutor-ral t√∂rt√©n≈ë h√≠v√°shoz.
    Minden batch saj√°t session-nel rendelkezik az event loop probl√©m√°k elker√ºl√©s√©re.
    """
    async def _run():
        # Optimaliz√°lt connector minden batch-hez
        connector = aiohttp.TCPConnector(
            limit=30,
            limit_per_host=30,
            force_close=False,
            keepalive_timeout=60,
        )
        timeout = aiohttp.ClientTimeout(total=300, connect=10, sock_read=300)
        
        # √öj session a batch-hez
        async with aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={'Connection': 'keep-alive'}
        ) as session:
            return await async_get_batch_llm_decision(session, articles_batch)
    
    # √öj event loop l√©trehoz√°sa a thread-ben
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        return loop.run_until_complete(_run())
    finally:
        loop.close()

def get_llm_decision_with_validation(description: str) -> Dict[str, Any]:
    """
    Szinkron wrapper az async_get_llm_decision_with_validation f√ºggv√©nyhez.
    Minden h√≠v√°s saj√°t session-nel rendelkezik.
    """
    async def _run():
        # Optimaliz√°lt connector
        connector = aiohttp.TCPConnector(
            limit=10,
            force_close=False,
            keepalive_timeout=60,
        )
        timeout = aiohttp.ClientTimeout(total=300, connect=10, sock_read=300)
        
        # √öj session
        async with aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={'Connection': 'keep-alive'}
        ) as session:
            return await async_get_llm_decision_with_validation(session, description)
    
    # √öj event loop l√©trehoz√°sa a thread-ben
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        return loop.run_until_complete(_run())
    finally:
        loop.close()

def worker_filter_article(row: pd.Series) -> Dict[str, Any]:
    """
    Gyors worker el≈ësz≈±r√©s - csak eld√∂nti hogy relev√°ns-e vagy sem.
    Ha bizonytalan, akkor 'needs_llm' = True
    """
    try:
        # Input valid√°ci√≥
        property_input = PropertyInput(**row.to_dict())
        article_id = property_input.article_id
        description = property_input.description or ""
        title = property_input.title or ""
        
    except Exception as e:
        return {
            'article_id': row.get('article_id', 'unknown'),
            'relevant': False,
            'reason': f'Valid√°ci√≥s hiba: {str(e)}',
            'needs_llm': False
        }
    
    # 1. √úres le√≠r√°s ‚Üí azonnal irrelev√°ns
    if not description or len(description.strip()) < 20:
        return {
            'article_id': article_id,
            'relevant': False,
            'reason': 'Worker el≈ësz≈±r√©s: √úres vagy t√∫l r√∂vid le√≠r√°s',
            'needs_llm': False
        }
    
    # 2. Kulcsszavas el≈ësz≈±r√©s ‚Üí azonnal irrelev√°ns  
    combined_text = f"{title} {description}".lower()
    
    # Speci√°lis eset: osztatlan k√∂z√∂s tulajdon CSAK ha t√∂rtsz√°mmal (x/y r√©sz)
    # P√©lda: "osztatlan k√∂z√∂s tulajdon 475/1000-d r√©sze" ‚Üí SZ≈∞R
    # De: "osztatlan k√∂z√∂s kertr√©sz" ‚Üí NEM SZ≈∞R (relev√°ns)
    osztatlan_pattern = r'osztatlan k√∂z√∂s tulajdon.*?\d+/\d+'
    if re.search(osztatlan_pattern, combined_text):
        return {
            'article_id': article_id,
            'relevant': False,
            'reason': 'Worker el≈ësz≈±r√©s: Osztatlan k√∂z√∂s tulajdon (t√∂rtsz√°m)',
            'needs_llm': False
        }
    
    # Egy√©rtelm≈± kiz√°r√≥ kulcsszavak
    for keyword, reason in DEFINITELY_IRRELEVANT_KEYWORDS.items():
        if re.search(r'\b' + re.escape(keyword) + r'\b', combined_text):
            # Ellen≈ërizz√ºk, hogy nincs-e neg√°ci√≥ a k√∂zelben
            # P√©ld√°ul: "nincs haszon√©lvezet", "tehermentes", "t√∂r√∂lve"
            negated = False
            for negation in NEGATION_KEYWORDS:
                # Keres√©s 50 karakteres ablakban a kulcssz√≥ el≈ëtt √©s ut√°n
                pattern = rf'.{{0,50}}\b{re.escape(negation)}\b.{{0,50}}\b{re.escape(keyword)}\b|\b{re.escape(keyword)}\b.{{0,50}}\b{re.escape(negation)}\b'
                if re.search(pattern, combined_text):
                    negated = True
                    break
            
            if not negated:
                return {
                    'article_id': article_id,
                    'relevant': False,
                    'reason': f'Worker el≈ësz≈±r√©s: {reason}',
                    'needs_llm': False
                }
    
    # 3. ML-alap√∫ el≈ësz≈±r√©s KIKAPCSOLVA TESZT M√ìDBAN
    # Csak a kulcsszavas sz≈±r√©sre koncentr√°lunk
    # if len(description.strip()) < 100:
    #     return {
    #         'article_id': article_id,
    #         'relevant': None,  # Bizonytalan
    #         'reason': 'Worker: R√∂vid le√≠r√°s - LLM pontos elemz√©sre v√°r',
    #         'needs_llm': True
    #     }
    # 
    # ml_filter = get_ml_filter()
    # if ml_filter.is_trained:
    #     ml_relevant, ml_confidence, ml_reason = ml_filter.predict(description)
    #     
    #     # Ha ML magabiztosan d√∂nt√∂tt ‚Üí haszn√°ljuk
    #     if ml_relevant is not None:
    #         return {
    #             'article_id': article_id,
    #             'relevant': ml_relevant,
    #             'reason': f'Worker ML sz≈±r√©s: {ml_reason}',
    #             'needs_llm': False
    #         }
    #     # ML bizonytalan ‚Üí LLM-re b√≠zza
    
    # 4. Ha kulcsszavas sz≈±r√©s nem d√∂nt√∂tt ‚Üí LLM-re b√≠zza
    return {
        'article_id': article_id,
        'relevant': None,  # Bizonytalan
        'reason': 'Worker nem tudta eld√∂nteni - LLM sz√ºks√©ges',
        'needs_llm': True
    }

def process_article_with_llm(row: pd.Series) -> Dict[str, Any]:
    """LLM feldolgoz√°s egy cikkhez (m√°r worker √°ltal j√≥v√°hagyott)."""
    try:
        property_input = PropertyInput(**row.to_dict())
        article_id = property_input.article_id
        description = property_input.description or ""
        title = property_input.title or ""
        
    except Exception as e:
        return {
            'article_id': row.get('article_id', 'unknown'),
            'relevant': False,
            'reason': f'Valid√°ci√≥s hiba: {str(e)}',
            'description': row.get('description', ''),
            'floor': None, 'street': None, 'building_type': None,
            'property_category': None, 'has_terrace': None
        }
    
    # LLM elemz√©s
    combined_text = f"{title} {description}"
    print(f"ü§ñ LLM elemz√©s: {article_id}", flush=True)
    llm_result = get_llm_decision_with_validation(combined_text)
    
    return {
        'article_id': article_id,
        'relevant': llm_result.get('relevant', False),
        'reason': f"LLM elemz√©s: {llm_result.get('reason', 'Nincs indokl√°s')}",
        'description': description,
        'filtered_by': 'llm',
        'floor': llm_result.get('floor'),
        'street': llm_result.get('street'),
        'building_type': llm_result.get('building_type'),
        'property_category': llm_result.get('property_category'),
        'has_terrace': llm_result.get('has_terrace')
    }

def process_article_enhanced(row: pd.Series, task_manager: TaskManager, task_id: str) -> Dict[str, Any]:
    """Tov√°bbfejlesztett hirdet√©sfeldolgoz√°s - DEPRECATED, haszn√°ld a k√©tf√°zis√∫ megk√∂zel√≠t√©st!"""
    # Ez a funkci√≥ m√°r nem haszn√°lt, megtartjuk backward compatibility miatt
    worker_result = worker_filter_article(row)
    if not worker_result['needs_llm']:
        return {
            'article_id': worker_result['article_id'],
            'relevant': worker_result['relevant'],
            'reason': worker_result['reason'],
            'description': row.get('description', ''),
            'filtered_by': 'worker',
            'floor': None, 'street': None, 'building_type': None,
            'property_category': None, 'has_terrace': None
        }
    else:
        return process_article_with_llm(row)

def process_data_async(task_id: str, *args, **kwargs):
    """
    F≈ë aszinkron adatfeldolgoz√≥ f√ºggv√©ny.
    Ez fut a h√°tt√©rben RQ worker-ben.
    
    Args:
        task_id: A feladat azonos√≠t√≥ja
        *args, **kwargs: RQ √°ltal √°tadott extra param√©terek (figyelmen k√≠v√ºl hagyjuk)
    """
    # TaskManager Redis-s√≥ inicializ√°l√°sa (worker k√∂rnyezetben nincs SocketIO)
    task_manager = TaskManager(socketio=None)
    
    try:
        # Cache kapcsolat tesztel√©se
        print("üîç Cache rendszer ellen≈ërz√©se...", flush=True)
        cache_ok = test_cache_connection()
        if cache_ok:
            cache_stats = get_cache_stats()
            print(f"üíæ Cache √°llapot: {cache_stats['cached_items']} t√°rolt elem, {cache_stats['memory_used_mb']} MB", flush=True)
        
        # Connection pool inicializ√°l√°s
        print("üöÑ HTTP Connection Pool inicializ√°l√°sa...", flush=True)
        pool_stats = get_connection_pool_stats()
        if pool_stats.get('active'):
            print(f"‚úÖ Connection pool akt√≠v: limit={pool_stats.get('limit', 'N/A')}, per_host={pool_stats.get('limit_per_host', 'N/A')}", flush=True)
        
        # Inkrement√°lis feldolgoz√°s inicializ√°l√°sa
        print("üîÑ Inkrement√°lis feldolgoz√°s ellen≈ërz√©se...", flush=True)
        incremental = get_incremental_processor()
        inc_stats = incremental.get_stats()
        if inc_stats['last_processing_date']:
            print(f"üìÖ Utols√≥ feldolgoz√°s: {inc_stats['last_processing_date']}", flush=True)
            print(f"üìä T√°rolt cikkek: {inc_stats['total_articles_tracked']}", flush=True)
        else:
            print("üÜï Els≈ë feldolgoz√°s - minden cikk feldolgoz√°sra ker√ºl", flush=True)
        
        # ML Worker Filter tr√©ning (ha van el√©g adat)
        # TESZT M√ìDBAN KIKAPCSOLVA - ne haszn√°lja a r√©gi 10k adatokat
        if not TEST_MODE:
            print("üéØ ML Worker Filter inicializ√°l√°sa...", flush=True)
            ml_trained = train_ml_filter_from_llm_log()
            if ml_trained:
                ml_stats = get_ml_filter().get_stats()
                print(f"‚úÖ ML filter akt√≠v: {ml_stats['relevant_samples']} relev√°ns, {ml_stats['irrelevant_samples']} irrelev√°ns minta", flush=True)
            else:
                print("‚ö†Ô∏è ML filter inakt√≠v (nincs el√©g tr√©ningadat)", flush=True)
        else:
            print("üß™ TESZT M√ìD: ML Worker Filter kikapcsolva (ne haszn√°lja a r√©gi adatokat)", flush=True)
        
        
        task_manager.update_progress(task_id, 0.0, "Feladat ind√≠t√°sa...")
        task_manager.set_status(task_id, "running", "Modell ellen≈ërz√©se...")
        
        # Modell pull (ha m√©g nincs let√∂ltve)
        try:
            # Ellen≈ërizz√ºk, hogy a modell m√°r l√©tezik-e
            available_models = ollama.list()
            model_exists = any(MODEL_NAME in model.get('name', '') for model in available_models.get('models', []))
            
            if not model_exists:
                print(f"üì• Modell let√∂lt√©se: {MODEL_NAME}", flush=True)
                ollama.pull(MODEL_NAME)
                print(f"‚úÖ Modell let√∂ltve: {MODEL_NAME}", flush=True)
            else:
                print(f"‚úÖ Modell m√°r el√©rhet≈ë: {MODEL_NAME}", flush=True)
        except Exception as e:
            # Ha nincs internet vagy m√°r let√∂ltve van, folytassuk
            print(f"‚ö†Ô∏è  Modell ellen≈ërz√©si hiba (folytat√°s): {e}", flush=True)
        
        task_manager.update_progress(task_id, 0.0, "Adatok bet√∂lt√©se...")

        # Adatok bet√∂lt√©se √©s valid√°l√°sa
        if not os.path.exists(INPUT_FILE):
            raise FileNotFoundError(f"Input f√°jl nem tal√°lhat√≥: {INPUT_FILE}")
        
        # üîç Parquet f√°jl elemz√©se streaming m√≥dban
        print("üìä Parquet f√°jl elemz√©se...", flush=True)
        file_info = estimate_parquet_memory(INPUT_FILE)
        print(f"   F√°jl m√©ret: {file_info['file_size_mb']} MB", flush=True)
        print(f"   Sorok sz√°ma: {file_info['total_rows']}", flush=True)
        print(f"   Becs√ºlt mem√≥ria: {file_info['estimated_memory_mb']} MB", flush=True)
        print(f"   Aj√°nlott chunk m√©ret: {file_info['recommended_chunk_size']}", flush=True)
        
        # M√°r feldolgozott elemek bet√∂lt√©se
        existing_processed = load_existing_results()
        existing_ids = set(existing_processed.keys())
        
        # üíæ Streaming unique articles - chunked processing
        print("üîÑ Unique cikkek bet√∂lt√©se streaming m√≥dban...", flush=True)
        all_unique_articles = get_unique_articles_streaming(
            INPUT_FILE,
            article_id_column='article_id',
            chunk_size=file_info['recommended_chunk_size'],
            exclude_ids=set()  # Ne sz≈±rj√ºk ki semmit - inkrement√°lis sz≈±r√©s k√©s≈ëbb
        )
        
        # üîÑ Inkrement√°lis sz≈±r√©s: csak √∫j/m√≥dosult cikkek
        print("üîç Inkrement√°lis sz≈±r√©s alkalmaz√°sa...", flush=True)
        unique_articles, new_checksums = incremental.filter_new_and_changed(
            all_unique_articles,
            timestamp_column='delivery_day',
            force_reprocess=False  # True-ra √°ll√≠tva minden cikket √∫jrafeldolgoz
        )
        
        # Valid√°ci√≥ egy kis mint√°n (els≈ë 100 sor)
        if len(unique_articles) > 0:
            sample_df = unique_articles.head(100)
            validate_dataframe_schema(sample_df)
        
        total_articles = len(all_unique_articles)  # √ñsszes unique cikk
        articles_to_process = unique_articles  # Csak √∫j/m√≥dosult
        
        # üß™ TESZT M√ìD: Csak els≈ë N hirdet√©st feldolgozni
        if TEST_MODE:
            print(f"üß™ TESZT M√ìD AKT√çV: Csak els≈ë {TEST_LIMIT} hirdet√©st dolgozunk fel", flush=True)
            articles_to_process = articles_to_process.head(TEST_LIMIT)
        
        total_to_process = len(articles_to_process)
        already_processed = len(existing_processed)
        
        # Kezdeti progress: m√°r feldolgozott / √∂sszes
        if total_articles > 0:
            initial_progress = (already_processed / total_articles) * 100
            task_manager.update_progress(
                task_id, initial_progress, 
                f"Bet√∂ltve: {already_processed}/{total_articles} m√°r k√©sz, {total_to_process} feldolgozand√≥",
                processed_items=already_processed,
                relevant_found=0,
                irrelevant_found=0,
                total_items=total_articles  # √ñsszes elem be√°ll√≠t√°sa!
            )
        else:
            task_manager.update_progress(
                task_id, 0.0, "Nincs feldolgozand√≥ adat",
                total_items=0
            )
        
        if len(articles_to_process) == 0:
            task_manager.update_progress(task_id, 100.0, "Minden hirdet√©s m√°r feldolgozott")
            task_manager.mark_completed(task_id, "Nincsenek √∫j hirdet√©sek feldolgoz√°sra")
            return
        
        print(f"üìä K√âTF√ÅZIS√ö FELDOLGOZ√ÅS KEZD√âS:", flush=True)
        print(f"   Feldolgozand√≥ cikkek: {total_to_process}", flush=True)
        print(f"   M√°r k√©sz: {already_processed}", flush=True)
        
        # ============ 1. F√ÅZIS: WORKER EL≈êSZ≈∞R√âS (GYORS, SZEKVENCI√ÅLIS) ============
        print(f"üîç 1. F√ÅZIS: Worker el≈ësz≈±r√©s kezd√©se...", flush=True)
        
        worker_results = []
        articles_for_llm = []
        worker_filtered_count = 0
        worker_relevant_count = 0
        
        for i, (_, article) in enumerate(articles_to_process.iterrows()):
            worker_result = worker_filter_article(article)
            
            if worker_result['needs_llm']:
                # LLM-re van sz√ºks√©g
                articles_for_llm.append(article)
            else:
                # Worker eld√∂nt√∂tte
                if worker_result['relevant']:
                    worker_relevant_count += 1
                else:
                    worker_filtered_count += 1
                
                # Eredm√©ny t√°rol√°sa
                final_result = {
                    'article_id': worker_result['article_id'],
                    'relevant': worker_result['relevant'],
                    'reason': worker_result['reason'],
                    'description': article.get('description', ''),
                    'filtered_by': 'worker',
                    'floor': None, 'street': None, 'building_type': None,
                    'property_category': None, 'has_terrace': None
                }
                worker_results.append(final_result)
            
            # Progress update statisztik√°kkal
            if i % 10 == 0 or i == total_to_process - 1:  # Minden 10. elemn√©l update
                phase1_progress = ((i + 1) / total_to_process) * 50.0  # 1. f√°zis 0-50%
                current_processed = already_processed + len(worker_results)
                print(f"üìä Worker f√°zis friss√≠t√©s: {phase1_progress:.1f}% - Worker {i+1}/{total_to_process} | Relev√°ns: {worker_relevant_count}, Irrelev√°ns: {worker_filtered_count}", flush=True)
                task_manager.update_progress(
                    task_id, phase1_progress, 
                    f"1. f√°zis - Worker el≈ësz≈±r√©s: {i+1}/{total_to_process}",
                    processed_items=current_processed,
                    relevant_found=worker_relevant_count,
                    irrelevant_found=worker_filtered_count,
                    total_items=total_articles
                )
        
        print(f"‚úÖ 1. F√ÅZIS K√âSZ:", flush=True)
        print(f"   Worker √°ltal sz≈±rt (irrelev√°ns): {worker_filtered_count}", flush=True)
        print(f"   LLM elemz√©sre v√°r: {len(articles_for_llm)}", flush=True)
        
        # ============ 2. F√ÅZIS: LLM FELDOLGOZ√ÅS BATCH-EKBEN (LASS√ö, P√ÅRHUZAMOS) ============
        print(f"üöÄ 2. F√ÅZIS: LLM elemz√©s kezd√©se ({len(articles_for_llm)} cikk, egyenk√©nt)...", flush=True)
        
        llm_results = []
        llm_processed_count = 0
        llm_relevant_count = 0
        
        if len(articles_for_llm) > 0:
            # BATCH PROCESSING KIKAPCSOLVA - egyenk√©nt dolgozzuk fel
            # A batch processing keveri √∂ssze a cikkeket (confusion)
            for article in articles_for_llm:
                individual_result = process_article_with_llm(article)
                llm_results.append(individual_result)
                llm_processed_count += 1
                if individual_result['relevant']:
                    llm_relevant_count += 1
            
            print(f"   Egyenk√©nt feldolgozva: {llm_processed_count} cikk", flush=True)
        
        # Eredm√©nyek egyes√≠t√©se
        all_results = worker_results + llm_results
        
        # V√©gs≈ë statisztik√°k
        final_relevant_count = sum(1 for r in all_results if r['relevant'])
        final_irrelevant_count = len(all_results) - final_relevant_count
        final_processed_count = already_processed + len(all_results)
        
        # Eredm√©nyek ment√©se
        save_results(all_results)
        
        # üîÑ Inkrement√°lis metadata friss√≠t√©se
        print("üìù Inkrement√°lis metadata friss√≠t√©se...", flush=True)
        incremental.update_metadata(new_checksums, len(all_results))
        
        print(f"‚úÖ 2. F√ÅZIS K√âSZ:", flush=True)
        print(f"   LLM √°ltal feldolgozott: {llm_processed_count}", flush=True)
        
        # R√©szletes statisztik√°k ki√≠r√°sa
        print(f"üìä V√âGS≈ê STATISZTIK√ÅK:", flush=True)
        print(f"   √ñsszes feldolgozott: {len(all_results)}", flush=True)
        print(f"   Relev√°ns: {final_relevant_count}", flush=True)
        print(f"   Irrelev√°ns: {final_irrelevant_count}", flush=True)
        print(f"   Worker √°ltal sz≈±rt: {worker_filtered_count}", flush=True)
        print(f"   LLM √°ltal elemzett: {llm_processed_count}", flush=True)
        if len(all_results) > 0:
            print(f"   LLM hat√©konys√°g: {llm_processed_count}/{len(all_results)} ({100*llm_processed_count/len(all_results):.1f}%)", flush=True)
        
        # Cache statisztik√°k a feldolgoz√°s v√©g√©n
        final_cache_stats = get_cache_stats()
        print(f"üíæ CACHE STATISZTIK√ÅK:", flush=True)
        print(f"   T√°rolt elemek: {final_cache_stats['cached_items']}", flush=True)
        print(f"   Mem√≥ria haszn√°lat: {final_cache_stats['memory_used_mb']} MB", flush=True)
        print(f"   TTL: {final_cache_stats['ttl_hours']} √≥ra", flush=True)
        
        # V√©gs≈ë progress 100%-kal √©s statisztik√°kkal
        task_manager.update_progress(
            task_id, 100.0, 
            "Feldolgoz√°s befejezve!",
            processed_items=final_processed_count,
            relevant_found=final_relevant_count,
            irrelevant_found=final_irrelevant_count,
            total_items=total_articles
        )
        task_manager.mark_completed(
            task_id, 
            f"‚úÖ K√©tf√°zis√∫ feldolgoz√°s k√©sz! Worker sz≈±rt: {worker_filtered_count}, LLM elemzett: {llm_processed_count}, Relev√°ns: {final_relevant_count}"
        )
        
    except Exception as e:
        task_manager.mark_failed(task_id, str(e))
        raise

def load_existing_results() -> Dict[str, Dict]:
    """Kor√°bban feldolgozott eredm√©nyek bet√∂lt√©se."""
    existing = {}
    
    # Relev√°ns eredm√©nyek
    if os.path.exists(OUTPUT_FILE):
        relevant_df = pd.read_parquet(OUTPUT_FILE)
        for _, row in relevant_df.iterrows():
            existing[row['article_id']] = {'relevant': True, 'data': row.to_dict()}
    
    # Irrelev√°ns eredm√©nyek  
    if os.path.exists(IRRELEVANT_OUTPUT_FILE):
        irrelevant_df = pd.read_parquet(IRRELEVANT_OUTPUT_FILE)
        for _, row in irrelevant_df.iterrows():
            existing[row['article_id']] = {'relevant': False, 'data': row.to_dict()}
    
    return existing

def save_results(results: List[Dict], input_file_path: str = INPUT_FILE):
    """
    Feldolgoz√°si eredm√©nyek ment√©se + Human feedback CSV.
    Streaming m√≥dban dolgozik - nem t√∂lti be a teljes eredeti DataFrame-et.
    """
    print(f"üíæ save_results kezd√©se - {len(results)} eredm√©ny feldolgoz√°sa", flush=True)
    
    # Relev√°ns √©s irrelev√°ns eredm√©nyek sz√©tv√°laszt√°sa
    relevant_results = [r for r in results if r['relevant']]
    irrelevant_results = [r for r in results if not r['relevant']]
    
    # üîÑ Streaming: csak a feldolgozott article_id-kat keress√ºk meg
    processed_ids = set(r['article_id'] for r in results)
    
    print(f"üíæ Eredm√©nyek ment√©se streaming m√≥dban ({len(processed_ids)} cikk)...", flush=True)
    
    # Streaming olvas√°s - csak a relev√°ns sorokat gy≈±jtj√ºk
    relevant_rows = []
    irrelevant_rows = []
    
    reader = ParquetStreamReader(input_file_path, chunk_size=50000)
    for chunk in reader.iter_batches_pyarrow(batch_size=50000):
        # Sz≈±r√©s: csak a feldolgozott cikkek
        processed_chunk = chunk[chunk['article_id'].isin(processed_ids)]
        
        if len(processed_chunk) > 0:
            # Relev√°ns √©s irrelev√°ns sorok sz√©tv√°laszt√°sa
            for _, row in processed_chunk.iterrows():
                article_id = row['article_id']
                
                # Megkeress√ºk a result-ot
                matching_result = next((r for r in results if r['article_id'] == article_id), None)
                
                if matching_result:
                    # Extra mez≈ëk hozz√°ad√°sa
                    row_dict = row.to_dict()
                    if matching_result['relevant']:
                        row_dict['reason'] = matching_result.get('reason', '')
                        row_dict['floor'] = matching_result.get('floor')
                        row_dict['street'] = matching_result.get('street')
                        row_dict['building_type'] = matching_result.get('building_type')
                        row_dict['property_category'] = matching_result.get('property_category')
                        row_dict['has_terrace'] = matching_result.get('has_terrace')
                        relevant_rows.append(row_dict)
                    else:
                        row_dict['reason_to_relevance'] = matching_result.get('reason', '')
                        irrelevant_rows.append(row_dict)
    
    # DataFrame-ek l√©trehoz√°sa √©s ment√©se
    if relevant_rows:
        relevant_df = pd.DataFrame(relevant_rows)
        
        # Append m√≥dban ment√©s (ha m√°r l√©tezik a f√°jl)
        if os.path.exists(OUTPUT_FILE):
            existing_relevant = pd.read_parquet(OUTPUT_FILE)
            relevant_df = pd.concat([existing_relevant, relevant_df], ignore_index=True)
            # Duplik√°tumok elt√°vol√≠t√°sa
            relevant_df = relevant_df.drop_duplicates(subset=['article_id'], keep='last')
        
        relevant_df.to_parquet(OUTPUT_FILE, index=False)
        print(f"‚úÖ {len(relevant_rows)} relev√°ns sor mentve", flush=True)
    
    if irrelevant_rows:
        irrelevant_df = pd.DataFrame(irrelevant_rows)
        
        # Append m√≥dban ment√©s
        if os.path.exists(IRRELEVANT_OUTPUT_FILE):
            existing_irrelevant = pd.read_parquet(IRRELEVANT_OUTPUT_FILE)
            irrelevant_df = pd.concat([existing_irrelevant, irrelevant_df], ignore_index=True)
            # Duplik√°tumok elt√°vol√≠t√°sa
            irrelevant_df = irrelevant_df.drop_duplicates(subset=['article_id'], keep='last')
        
        irrelevant_df.to_parquet(IRRELEVANT_OUTPUT_FILE, index=False)
        print(f"‚úÖ {len(irrelevant_rows)} irrelev√°ns sor mentve", flush=True)
    
    # Human feedback CSV l√©trehoz√°sa
    try:
        print(f"üìù Human feedback CSV k√©sz√≠t√©se - {len(results)} cikk feldolgoz√°sa...", flush=True)
        feedback_data = []
        for result in results:
            if result is None:
                print(f"‚ö†Ô∏è None result found, skipping...", flush=True)
                continue
                
            article_id = result.get('article_id', 'unknown')
            description = result.get('description') or ''  # Handle None
            description = description[:500] if description else ''  # Els≈ë 500 karakter
            relevant = result.get('relevant', False)
            reason = result.get('reason', '')
            filtered_by = result.get('filtered_by', 'unknown')
            
            feedback_data.append({
                'article_id': article_id,
                'description_preview': description,
                'llm_relevant': relevant,
                'llm_reason': reason,
                'filtered_by': filtered_by,
                'human_feedback': ''  # √úres oszlop human feedback-hez
            })
        
        print(f"üìù {len(feedback_data)} feedback bejegyz√©s el≈ëk√©sz√≠tve", flush=True)
        
        # Human feedback XLSX ment√©se (szerkeszthet≈ë Excel form√°tum)
        feedback_df = pd.DataFrame(feedback_data)
        feedback_xlsx_path = '/workspace/human_feedback.xlsx'
        
        # Ha m√°r l√©tezik, hozz√°f≈±z√©s
        if os.path.exists(feedback_xlsx_path):
            existing_df = pd.read_excel(feedback_xlsx_path, engine='openpyxl')
            # Duplik√°tum elker√ºl√©se: csak azok amelyek m√©g nincsenek benne
            existing_ids = set(existing_df['article_id'].values)
            new_feedback = feedback_df[~feedback_df['article_id'].isin(existing_ids)]
            if len(new_feedback) > 0:
                combined_df = pd.concat([existing_df, new_feedback], ignore_index=True)
                combined_df.to_excel(feedback_xlsx_path, index=False, engine='openpyxl')
                print(f"üìù Human feedback XLSX friss√≠tve: +{len(new_feedback)} √∫j cikk (√∂ssz: {len(combined_df)})", flush=True)
            else:
                print(f"üìù Human feedback XLSX m√°r naprak√©sz (nincs √∫j cikk)", flush=True)
        else:
            feedback_df.to_excel(feedback_xlsx_path, index=False, engine='openpyxl')
            print(f"üìù Human feedback XLSX l√©trehozva: {len(feedback_data)} cikk - {feedback_xlsx_path}", flush=True)
    
    except Exception as e:
        print(f"‚ùå HIBA a human feedback CSV l√©trehoz√°s√°n√°l: {str(e)}", flush=True)
        import traceback
        traceback.print_exc()
    
    # Log f√°jl friss√≠t√©se
    try:
        log_df = pd.DataFrame(results)
        log_df.to_csv(LOG_FILE, index=False)
        print(f"üìä LLM decisions log friss√≠tve: {len(results)} bejegyz√©s", flush=True)
    except Exception as e:
        print(f"‚ùå HIBA a log f√°jl friss√≠t√©s√©n√©l: {str(e)}", flush=True)